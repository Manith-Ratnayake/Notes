
Stemming Vs Lemmatization in Data Science
Stemming
• Process of removing last few characters from a word, often leading to incorrect meanings and spelling.
Lemmatization
• Considers the context and converts the word to its meaningful base form, which is called Lemma.


Segmentation
The process of dividing a sentence into its component sentences, usually along the punctutation marks

Tokenization
The process of splitting sentences into their constitute words 

Stemming

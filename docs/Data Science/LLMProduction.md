Hidden Markov models (HMM)
markov chain essentially add state to the N-gram models, storing probabilities using hidden state.

Attention
is a mathematical shortcut that gives the model a mechanism for solving larger context windows faster by telling the model through an emergent mathematical formular which parts of an input to consider and how much.
Instead of key-pair (dictionary) contextual query is added.


The dot product attention - captures the relationship between each word in the query and every word in the key
When queries and keys are part of the same sentences known as 'bi-directional self attention'
Queries and keys comes from same sentence is casual attention


Convolutions are good in computer vision, not in NLP
LLM are the solution for the NLP



